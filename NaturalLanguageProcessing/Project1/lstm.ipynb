{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM-Model for Text Generation\n",
    "\n",
    "Some reference on LSTMs:\n",
    "* Colah, <https://colah.github.io/posts/2015-08-Understanding-LSTMs/>\n",
    "* Pytorch tutorial, <https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html>\n",
    "* Text Generation With Pytorch, <https://machinetalk.org/2019/02/08/text-generation-with-pytorch/>\n",
    "* Language Modelling and Text Generation using LSTMs — Deep Learning for NLP, <https://medium.com/@shivambansal36/language-modelling-text-generation-using-lstms-deep-learning-for-nlp-ed36b224b275>\n",
    "\n",
    "![Text generation](https://machinetalk.org/wp-content/uploads/2019/02/predict.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "from collections import Counter\n",
    "from argparse import Namespace\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(level = logging.INFO)\n",
    "handler = logging.FileHandler(\"lstm-6.log\")\n",
    "handler.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "handler.setFormatter(formatter)\n",
    "logger.addHandler(handler)\n",
    "\n",
    "flags = Namespace(\n",
    "    train_file_path='dict_no_stop_jieba',\n",
    "    checkpoint_path='checkpoint',\n",
    "    seq_size=32,\n",
    "    batch_size=64,\n",
    "    embedding_size=128, # embedding dimension\n",
    "    lstm_size=128, # hidden dimension\n",
    "    gradients_norm=5, # gradient clipping\n",
    "    top_k=5,\n",
    "    num_epochs=40,\n",
    "    learning_rate=0.001\n",
    ")\n",
    "\n",
    "logger.info(str(flags))\n",
    "if not os.path.exists(flags.checkpoint_path):\n",
    "    os.mkdir(flags.checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import data\n",
    "\n",
    "class TextDataset(data.Dataset):\n",
    "    \"\"\"\n",
    "    My own text dataset\n",
    "    ref: https://stanford.edu/~shervine/blog/pytorch-how-to-generate-data-parallel\n",
    "    \"\"\"\n",
    "    def __init__(self, file_path, batch_size, seq_size):\n",
    "        \"\"\"\n",
    "        Generate word indices and dictionary used for training\n",
    "\n",
    "        file_path: The path of the folder storing all the news\n",
    "        batch_size: size of one batch (used for batch training)\n",
    "        seq_size: size of the sequence\n",
    "        \"\"\"\n",
    "        text = []\n",
    "        for i,file_name in enumerate(os.listdir(file_path),1):\n",
    "            with open(\"{}/{}\".format(file_path,file_name),\"r\",encoding=\"utf-8\") as infile:\n",
    "                for j,line in enumerate(infile):\n",
    "                    text += line.split()\n",
    "        word_counts = Counter(text) # {word: count}\n",
    "        # sort based on counts, but only remain the word strings\n",
    "        sorted_vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "\n",
    "        # make embedding based on the occurance frequency of the words\n",
    "        self.int_to_word = {k: w for k, w in enumerate(sorted_vocab)}\n",
    "        self.word_to_int = {w: k for k, w in self.int_to_word.items()}\n",
    "        self.n_word = len(self.int_to_word)\n",
    "        print('Vocabulary size', self.n_word)\n",
    "\n",
    "        # turn all the words in the text to int\n",
    "        int_text = [self.word_to_int[w] for w in text]\n",
    "        num_batches = int(len(int_text) / (seq_size * batch_size))\n",
    "        in_text = int_text[:num_batches * batch_size * seq_size]\n",
    "\n",
    "        # shift right for one position to generate the 'label' Y\n",
    "        out_text = np.zeros_like(in_text)\n",
    "        out_text[:-1] = in_text[1:]\n",
    "        out_text[-1] = in_text[0]\n",
    "\n",
    "        # reshape X and Y (# of seq,seq_size)\n",
    "        self.in_text = np.reshape(in_text,(-1,seq_size))\n",
    "        self.out_text = np.reshape(out_text,(-1,seq_size))\n",
    "        self.seq_size = seq_size\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return the total number of samples\n",
    "        \"\"\"\n",
    "        return len(self.in_text)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Generate one sample of the data\n",
    "        \"\"\"\n",
    "        x = self.in_text[idx]\n",
    "        y = self.out_text[idx]\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![LSTM Basic Unit](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModule(nn.Module):\n",
    "    \"\"\"\n",
    "    A basic LSTM model for text generation\n",
    "    \"\"\"\n",
    "    def __init__(self, n_word, seq_size, embedding_size, lstm_size):\n",
    "        super(RNNModule, self).__init__()\n",
    "        self.seq_size = seq_size\n",
    "        self.lstm_size = lstm_size\n",
    "        \n",
    "        # embed = nn.Embedding(vocab_size, vector_size)\n",
    "        # `vocab_size` is the number of words in your train, val and test set\n",
    "        # `vector_size` is the dimension of the word vectors you are using\n",
    "        # you can view it as a linear transformation\n",
    "        # the tensor is initialized randomly\n",
    "        self.embedding = nn.Embedding(n_word, embedding_size)\n",
    "        \n",
    "        self.lstm = nn.LSTM(embedding_size, lstm_size, batch_first=True)\n",
    "        self.linear = nn.Linear(lstm_size, n_word)\n",
    "\n",
    "    def forward(self, x, prev_state):\n",
    "        \"\"\"\n",
    "        Forward propagation\n",
    "        \"\"\"\n",
    "        embedding = self.embedding(x)\n",
    "        # used for next layer\n",
    "        output, state = self.lstm(embedding, prev_state)\n",
    "        # used for output\n",
    "        logits = self.linear(output)\n",
    "        return logits, state\n",
    "\n",
    "    def zero_state(self, batch_size):\n",
    "        \"\"\"\n",
    "        Used to make the state all zeros\n",
    "        \"\"\"\n",
    "        return (torch.zeros(1, batch_size, self.lstm_size),\n",
    "                torch.zeros(1, batch_size, self.lstm_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, sys\n",
    "\n",
    "def train():\n",
    "    \"\"\"\n",
    "    Core training function\n",
    "    \"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    train_set = TextDataset(flags.train_file_path, flags.batch_size, flags.seq_size)\n",
    "    train_loader = data.DataLoader(dataset=train_set,batch_size=flags.batch_size,shuffle=False)\n",
    "\n",
    "    net = RNNModule(train_set.n_word, flags.seq_size, flags.embedding_size, flags.lstm_size)\n",
    "    net = net.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=flags.learning_rate)\n",
    "\n",
    "    iteration = 0\n",
    "    losses = []\n",
    "\n",
    "    start_time = time.time()\n",
    "    for e in range(flags.num_epochs):\n",
    "        state_h, state_c = net.zero_state(flags.batch_size)\n",
    "        state_h = state_h.to(device)\n",
    "        state_c = state_c.to(device)\n",
    "        \n",
    "        for step, (x, y) in enumerate(train_loader):\n",
    "            iteration += 1\n",
    "            net.train()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            x = torch.tensor(x).to(device)\n",
    "            y = torch.tensor(y).to(device)\n",
    "\n",
    "            logits, (state_h, state_c) = net(x, (state_h, state_c))\n",
    "            loss = criterion(logits.transpose(1, 2), y)\n",
    "\n",
    "            loss_value = loss.item()\n",
    "\n",
    "            # avoid delivering loss from h_t and c_t\n",
    "            # thus need to remove them from the computation graph\n",
    "            state_h = state_h.detach()\n",
    "            state_c = state_c.detach()\n",
    "            \n",
    "            loss.backward()\n",
    "\n",
    "            # avoid gradient explosion\n",
    "            _ = torch.nn.utils.clip_grad_norm_(net.parameters(), flags.gradients_norm)\n",
    "\n",
    "            optimizer.step()\n",
    "            losses.append(loss_value)\n",
    "\n",
    "            if iteration % 100 == 0:\n",
    "                print('Epoch: {}/{}'.format(e+1, flags.num_epochs),\n",
    "                      'Iteration: {}'.format(iteration),\n",
    "                      'Loss: {}'.format(loss_value))\n",
    "                logger.info('Epoch: {}/{} Iteration: {} Loss: {}'.format(e+1, flags.num_epochs, iteration, loss_value))\n",
    "\n",
    "            if iteration % 1000 == 0:\n",
    "                torch.save(net.state_dict(),\n",
    "                           '{}/model-{}.pth'.format(flags.checkpoint_path,iteration))\n",
    "\n",
    "    print(\"Time:{}s\".format(time.time()-start_time))\n",
    "    torch.save(net.state_dict(),'{}/model-{}.pth'.format(flags.checkpoint_path,\"final\"))\n",
    "    return net, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    net, losses = train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The following part is used for prediction and generating the answers\n",
    "\"\"\"\n",
    "\n",
    "import jieba\n",
    "\n",
    "stopwords = [word[:-1] for word in open(\"stopwords.txt\",\"r\",encoding=\"utf-8\")] # delete \\n\n",
    "\n",
    "def generate_seg_lst(lst):\n",
    "    cut_lst = jieba.lcut(lst,cut_all=False)\n",
    "    res = []\n",
    "    for word in cut_lst:\n",
    "        if word not in stopwords and word != \"\\n\":\n",
    "            res.append(word)\n",
    "    return res\n",
    "\n",
    "def predict(device, net, question_str, n_word, word_to_int, int_to_word, top_k=5):\n",
    "    \"\"\"\n",
    "    Use `net` to do the prediction\n",
    "    Each time only one `question_str` is input\n",
    "    \"\"\"\n",
    "    net.eval() # set in evaluation mode\n",
    "    # find out the blank\n",
    "    q_index = question_str.index(\"[MASK]\")\n",
    "    question_pre, question_post = question_str[:q_index], question_str[q_index+len(\"[MASK]\"):]\n",
    "\n",
    "    # cut the sentence\n",
    "    seg_pre = generate_seg_lst(question_pre)\n",
    "    seg_post = generate_seg_lst(question_post)\n",
    "    seg_pre.insert(0,\"<BOS>\")\n",
    "    seg_post.insert(len(seg_post),\"<EOS>\")\n",
    "\n",
    "    # LSTM inference\n",
    "    state_h, state_c = net.zero_state(1)\n",
    "    state_h = state_h.to(device)\n",
    "    state_c = state_c.to(device)\n",
    "    for w in seg_pre:\n",
    "        index = word_to_int.get(w,word_to_int[\"<BOS>\"])\n",
    "        ix = torch.tensor([[index]]).to(device)\n",
    "        output, (state_h, state_c) = net(ix, (state_h, state_c))\n",
    "\n",
    "    # get the topk prediction\n",
    "    _, top_ix = torch.topk(output[0], k=top_k)\n",
    "    choices = top_ix.tolist()\n",
    "\n",
    "    # return the corresponding words\n",
    "    return [int_to_word[x] for x in choices[0]]\n",
    "\n",
    "top_k = 5 # flags.top_k\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "train_set = TextDataset(flags.train_file_path, flags.batch_size, flags.seq_size)\n",
    "\n",
    "# load from previous models\n",
    "net = RNNModule(train_set.n_word, flags.seq_size,flags.embedding_size, flags.lstm_size)\n",
    "net.load_state_dict(torch.load(\"checkpoint/model-final.pth\"))\n",
    "net.to(device)\n",
    "\n",
    "groundtrue = [line[:-1] for line in open(\"answer.txt\",\"r\",encoding=\"utf-8\")]\n",
    "acc = 0\n",
    "\n",
    "myanswer = open(\"myanswer-lstm-{}.txt\".format(top_k),\"w\",encoding=\"utf-8\")\n",
    "\n",
    "print(\"Use LSTM model to predict\")\n",
    "with open(\"questions.txt\",\"r\",encoding=\"utf-8\") as question_file:\n",
    "    for i,question_str in enumerate(question_file,1):\n",
    "        pred = predict(device, net, question_str, train_set.n_word, train_set.word_to_int, train_set.int_to_word, top_k)\n",
    "        if groundtrue[i-1] in pred:\n",
    "            acc += 1\n",
    "            print(\"{}√ [MASK] = {} - {}\".format(i,pred,groundtrue[i-1]),flush=True)\n",
    "        else:\n",
    "            print(\"{} [MASK] = {} - {}\".format(i,pred,groundtrue[i-1]),flush=True)\n",
    "        myanswer.write(\"{}\\n\".format(\" \".join(pred)))\n",
    "\n",
    "print(\"Accuracy: {:.2f}%\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Plot the train loss curve\n",
    "\"\"\"\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.plot(losses)\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.savefig(r\"train_loss.pdf\",format=\"pdf\",dpi=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Plot the accuracy curve\n",
    "\"\"\"\n",
    "all_acc_1 = []\n",
    "all_acc_3 = []\n",
    "all_acc_5 = []\n",
    "for step in range(1000,17001,1000):\n",
    "    net = RNNModule(train_set.n_word, flags.seq_size,flags.embedding_size, flags.lstm_size)\n",
    "    net.load_state_dict(torch.load(\"checkpoint-5/model-{}.pth\".format(step)))\n",
    "    net.to(device)\n",
    "    acc_1, acc_3, acc_5 = 0, 0, 0\n",
    "    with open(\"questions.txt\",\"r\",encoding=\"utf-8\") as question_file:\n",
    "        for i,question_str in enumerate(question_file,1):\n",
    "            pred1 = predict(device, net, question_str, train_set.n_word, train_set.word_to_int, train_set.int_to_word, 1)\n",
    "            pred3 = predict(device, net, question_str, train_set.n_word, train_set.word_to_int, train_set.int_to_word, 3)\n",
    "            pred5 = predict(device, net, question_str, train_set.n_word, train_set.word_to_int, train_set.int_to_word, 5)\n",
    "            if groundtrue[i-1] in pred1:\n",
    "                acc_1 += 1\n",
    "            if groundtrue[i-1] in pred3:\n",
    "                acc_3 += 1\n",
    "            if groundtrue[i-1] in pred5:\n",
    "                acc_5 += 1\n",
    "    all_acc_1.append(acc_1 / 100)\n",
    "    all_acc_3.append(acc_3 / 100)\n",
    "    all_acc_5.append(acc_5 / 100)\n",
    "    print(\"Done model-{}.pth evaluation\".format(step))\n",
    "\n",
    "plt.plot(all_acc_1,label=\"Top 1\")\n",
    "plt.plot(all_acc_3,label=\"Top 3\")\n",
    "plt.plot(all_acc_5,label=\"Top 5\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Iteration (1k)\")\n",
    "plt.legend(loc=0)\n",
    "plt.savefig(r\"acc-iter.pdf\",format=\"pdf\",dpi=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
